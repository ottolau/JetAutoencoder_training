{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "broke-peace",
   "metadata": {},
   "source": [
    "import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf, re, math\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras import backend as K \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n",
    "import uproot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "    print(\"Running on TPU \", tpu.cluster_spec().as_dict()[\"worker\"])\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except ValueError:\n",
    "    print(\"Not connected to a TPU runtime. Using CPU/GPU strategy\")\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-lingerie",
   "metadata": {},
   "source": [
    "build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autoencoder(original_dim, intermediate_dim_1, intermediate_dim_2, latent_dim):\n",
    "    # encoder\n",
    "    encoder_inputs = layers.Input(shape=(original_dim,))\n",
    "    h = layers.Dense(intermediate_dim_1)(encoder_inputs)\n",
    "    h = layers.PReLU(alpha_initializer=tf.keras.initializers.Constant(value=0.25))(h)\n",
    "    h = layers.BatchNormalization()(h)\n",
    "    h = layers.Dropout(0.2)(h)\n",
    "    h = layers.Dense(intermediate_dim_2)(h)\n",
    "    h = layers.PReLU(alpha_initializer=tf.keras.initializers.Constant(value=0.25))(h)\n",
    "    h = layers.BatchNormalization()(h)\n",
    "    h = layers.Dropout(0.2)(h)\n",
    "    h = layers.Dense(latent_dim)(h)\n",
    "    encoder = Model(encoder_inputs, h, name=\"encoder\")\n",
    "\n",
    "    # decoder\n",
    "    decoder_inputs = layers.Input(shape=(latent_dim,))\n",
    "    d = layers.Dense(intermediate_dim_2)(decoder_inputs)\n",
    "    d = layers.PReLU(alpha_initializer=tf.keras.initializers.Constant(value=0.25))(d)\n",
    "    d = layers.BatchNormalization()(d)\n",
    "    d = layers.Dropout(0.2)(d)\n",
    "    d = layers.Dense(intermediate_dim_1)(d)\n",
    "    d = layers.PReLU(alpha_initializer=tf.keras.initializers.Constant(value=0.25))(d)\n",
    "    d = layers.BatchNormalization()(d)\n",
    "    d = layers.Dropout(0.2)(d)\n",
    "    d = layers.Dense(original_dim, activation='sigmoid')(d)\n",
    "    decoder = Model(decoder_inputs, d, name=\"decoder\")\n",
    "    \n",
    "    x_pred = decoder(h)\n",
    "    \n",
    "    vae = Model(inputs=encoder_inputs, outputs=x_pred, name='vae')\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-accessory",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(root_file_name, filter_name):\n",
    "    events = uproot.open(root_file_name, filter_name=filter_name)[\"tree\"]\n",
    "    df = events.arrays(library=\"pd\")\n",
    "    return df\n",
    "\n",
    "features = []\n",
    "# variables: general\n",
    "features += ['FatJet_pt', 'FatJet_eta', 'FatJet_phi', 'FatJet_DDX_jetNSecondaryVertices', \\\n",
    "             'FatJet_DDX_jetNTracks', 'FatJet_DDX_z_ratio', 'FatJet_Proba', 'FatJet_area', \\\n",
    "             'FatJet_jetId', 'FatJet_lsf3', 'FatJet_rawFactor', 'FatJet_n2b1', 'FatJet_n3b1', \\\n",
    "            ]\n",
    "\n",
    "# variables: tau1\n",
    "features += ['FatJet_tau1', 'FatJet_DDX_tau1_flightDistance2dSig', 'FatJet_DDX_tau1_trackEtaRel_0', \\\n",
    "             'FatJet_DDX_tau1_trackEtaRel_1', 'FatJet_DDX_tau1_trackEtaRel_2', 'FatJet_DDX_tau1_trackSip3dSig_0', \\\n",
    "             'FatJet_DDX_tau1_trackSip3dSig_1', 'FatJet_DDX_tau1_vertexDeltaR', 'FatJet_DDX_tau1_vertexEnergyRatio', \\\n",
    "            ]\n",
    "\n",
    "# variables: tau2\n",
    "features += ['FatJet_tau2', 'FatJet_DDX_tau2_flightDistance2dSig', 'FatJet_DDX_tau2_trackEtaRel_0', \\\n",
    "             'FatJet_DDX_tau2_trackEtaRel_1', 'FatJet_DDX_tau2_trackEtaRel_3', 'FatJet_DDX_tau2_trackSip3dSig_0', \\\n",
    "             'FatJet_DDX_tau2_trackSip3dSig_1', 'FatJet_DDX_tau2_vertexEnergyRatio', \\\n",
    "            ]\n",
    "\n",
    "# variables: tau3 and tau4\n",
    "features += ['FatJet_tau3', 'FatJet_tau4',]\n",
    "\n",
    "# variables: track\n",
    "features += ['FatJet_DDX_trackSip2dSigAboveBottom_0', 'FatJet_DDX_trackSip2dSigAboveBottom_1', \\\n",
    "             'FatJet_DDX_trackSip2dSigAboveCharm', 'FatJet_DDX_trackSip3dSig_0', \\\n",
    "             'FatJet_DDX_trackSip3dSig_1', 'FatJet_DDX_trackSip3dSig_2', 'FatJet_DDX_trackSip3dSig_3', \\\n",
    "            ]\n",
    "\n",
    "# variables: subjet 1\n",
    "features += ['FatJet_subjet1_pt', 'FatJet_subjet1_eta', 'FatJet_subjet1_phi', \\\n",
    "             'FatJet_subjet1_Proba', 'FatJet_subjet1_tau1', 'FatJet_subjet1_tau2', \\\n",
    "             'FatJet_subjet1_tau3', 'FatJet_subjet1_tau4', 'FatJet_subjet1_n2b1', 'FatJet_subjet1_n3b1', \\\n",
    "            ]\n",
    "\n",
    "# variables: subjet 2\n",
    "features += ['FatJet_subjet2_pt', 'FatJet_subjet2_eta', 'FatJet_subjet2_phi', \\\n",
    "             'FatJet_subjet2_Proba', 'FatJet_subjet2_tau1', 'FatJet_subjet2_tau2', \\\n",
    "             'FatJet_subjet2_tau3', 'FatJet_subjet2_tau4', 'FatJet_subjet2_n2b1', 'FatJet_subjet2_n3b1', \\\n",
    "            ]\n",
    "\n",
    "# variables: fatjet sv\n",
    "features += ['FatJet_sv_costhetasvpv', 'FatJet_sv_d3dsig', 'FatJet_sv_deltaR', 'FatJet_sv_dxysig', \\\n",
    "             'FatJet_sv_enration', 'FatJet_sv_normchi2', 'FatJet_sv_ntracks', 'FatJet_sv_phirel', \\\n",
    "             'FatJet_sv_pt', 'FatJet_sv_ptrel', \\\n",
    "            ]\n",
    "\n",
    "features = sorted(features)\n",
    "\n",
    "original_dim = len(features)\n",
    "\n",
    "inputfile = 'QCD_HT500to700.root'\n",
    "df = get_df(inputfile, '*')\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df = df[features]\n",
    "\n",
    "X = df.to_numpy()\n",
    "X = X.astype(\"float32\")\n",
    "\n",
    "# Scale our data using a MinMaxScaler that will scale \n",
    "# each number so that it will be between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "x_train, x_test = train_test_split(X, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dset(df): \n",
    "    df = df.copy()\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((df, df))\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "    \n",
    "x_train_dataset = build_dset(x_train)\n",
    "x_test_dataset = build_dset(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-swiss",
   "metadata": {},
   "source": [
    "get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_dim_1 = 32\n",
    "intermediate_dim_2 = 16\n",
    "latent_dim = 8\n",
    "\n",
    "with strategy.scope():\n",
    "    model = get_autoencoder(original_dim, intermediate_dim_1, intermediate_dim_2, latent_dim)\n",
    "    #model.compile(optimizer=tf.keras.optimizers.Adam(clipnorm=1.0), loss=tf.keras.losses.MeanSquaredError())\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.MeanSquaredError())\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-center",
   "metadata": {},
   "source": [
    "train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_callback():\n",
    "    lr_start   = 0.000001\n",
    "    lr_max     = 0.01\n",
    "    lr_min     = 0.000001\n",
    "    lr_ramp_ep = 5\n",
    "    lr_sus_ep  = 10\n",
    "    lr_decay   = 0.8\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start   \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max    \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "    return lr_callback\n",
    "\n",
    "checkpoint_path = \"weights.{epoch:05d}.hdf5\"\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 monitor = 'val_loss',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only=False,\n",
    "                                                 mode = 'min',\n",
    "                                                 verbose=1)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "history = model.fit(\n",
    "    x_train_dataset,\n",
    "    shuffle=True,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=x_test_dataset,\n",
    "    callbacks=[cp_callback, get_lr_callback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-centre",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
