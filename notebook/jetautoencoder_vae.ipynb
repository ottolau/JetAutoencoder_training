{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "robust-aircraft",
   "metadata": {},
   "source": [
    "import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-excerpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf, re, math\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras import backend as K \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n",
    "import uproot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "    print(\"Running on TPU \", tpu.cluster_spec().as_dict()[\"worker\"])\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except ValueError:\n",
    "    print(\"Not connected to a TPU runtime. Using CPU/GPU strategy\")\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-sight",
   "metadata": {},
   "source": [
    "loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-arkansas",
   "metadata": {},
   "source": [
    "build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(original_dim, intermediate_dim_1, intermediate_dim_2, latent_dim):\n",
    "    # Encoder\n",
    "    encoder_inputs = layers.Input(shape=(original_dim,))\n",
    "    h = layers.Dense(intermediate_dim_1)(encoder_inputs)\n",
    "    h = layers.PReLU(alpha_initializer=tf.keras.initializers.Constant(value=0.25))(h)\n",
    "    h = layers.BatchNormalization()(h)\n",
    "    h = layers.Dropout(0.2)(h)\n",
    "    h = layers.Dense(intermediate_dim_2)(h)\n",
    "    h = layers.PReLU(alpha_initializer=tf.keras.initializers.Constant(value=0.25))(h)\n",
    "    h = layers.BatchNormalization()(h)\n",
    "    h = layers.Dropout(0.2)(h)\n",
    "    z_mu = layers.Dense(latent_dim, name=\"z_mean\")(h)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(h)\n",
    "\n",
    "    z = Sampling()([z_mu, z_log_var])\n",
    "    encoder = Model(encoder_inputs, [z_mu, z_log_var, z], name=\"encoder\")\n",
    "    return encoder  \n",
    "    \n",
    "def get_decoder(original_dim, intermediate_dim_1, intermediate_dim_2, latent_dim):\n",
    "    decoder_inputs = layers.Input(shape=(latent_dim,))\n",
    "    d = layers.Dense(intermediate_dim_2)(decoder_inputs)\n",
    "    d = layers.PReLU(alpha_initializer=tf.keras.initializers.Constant(value=0.25))(d)\n",
    "    d = layers.BatchNormalization()(d)\n",
    "    d = layers.Dropout(0.2)(d)\n",
    "    d = layers.Dense(intermediate_dim_1)(d)\n",
    "    d = layers.PReLU(alpha_initializer=tf.keras.initializers.Constant(value=0.25))(d)\n",
    "    d = layers.BatchNormalization()(d)\n",
    "    d = layers.Dropout(0.2)(d)\n",
    "    d = layers.Dense(original_dim, activation='sigmoid')(d)\n",
    "    decoder = Model(decoder_inputs, d, name=\"decoder\")\n",
    "    return decoder\n",
    "\n",
    "class vae(Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(vae, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.encoder.summary()\n",
    "        self.decoder.summary()\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    tf.keras.losses.binary_crossentropy(data, reconstruction), axis=-1\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.encoder.trainable_weights + self.decoder.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.encoder.trainable_weights + self.decoder.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-billy",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(root_file_name, filter_name):\n",
    "    events = uproot.open(root_file_name, filter_name=filter_name)[\"tree\"]\n",
    "    df = events.arrays(library=\"pd\")\n",
    "    return df\n",
    "\n",
    "features = []\n",
    "# variables: general\n",
    "features += ['FatJet_pt', 'FatJet_eta', 'FatJet_phi', 'FatJet_DDX_jetNSecondaryVertices', \\\n",
    "             'FatJet_DDX_jetNTracks', 'FatJet_DDX_z_ratio', 'FatJet_Proba', 'FatJet_area', \\\n",
    "             'FatJet_jetId', 'FatJet_lsf3', 'FatJet_rawFactor', 'FatJet_n2b1', 'FatJet_n3b1', \\\n",
    "            ]\n",
    "\n",
    "# variables: tau1\n",
    "features += ['FatJet_tau1', 'FatJet_DDX_tau1_flightDistance2dSig', 'FatJet_DDX_tau1_trackEtaRel_0', \\\n",
    "             'FatJet_DDX_tau1_trackEtaRel_1', 'FatJet_DDX_tau1_trackEtaRel_2', 'FatJet_DDX_tau1_trackSip3dSig_0', \\\n",
    "             'FatJet_DDX_tau1_trackSip3dSig_1', 'FatJet_DDX_tau1_vertexDeltaR', 'FatJet_DDX_tau1_vertexEnergyRatio', \\\n",
    "            ]\n",
    "\n",
    "# variables: tau2\n",
    "features += ['FatJet_tau2', 'FatJet_DDX_tau2_flightDistance2dSig', 'FatJet_DDX_tau2_trackEtaRel_0', \\\n",
    "             'FatJet_DDX_tau2_trackEtaRel_1', 'FatJet_DDX_tau2_trackEtaRel_3', 'FatJet_DDX_tau2_trackSip3dSig_0', \\\n",
    "             'FatJet_DDX_tau2_trackSip3dSig_1', 'FatJet_DDX_tau2_vertexEnergyRatio', \\\n",
    "            ]\n",
    "\n",
    "# variables: tau3 and tau4\n",
    "features += ['FatJet_tau3', 'FatJet_tau4',]\n",
    "\n",
    "# variables: track\n",
    "features += ['FatJet_DDX_trackSip2dSigAboveBottom_0', 'FatJet_DDX_trackSip2dSigAboveBottom_1', \\\n",
    "             'FatJet_DDX_trackSip2dSigAboveCharm', 'FatJet_DDX_trackSip3dSig_0', \\\n",
    "             'FatJet_DDX_trackSip3dSig_1', 'FatJet_DDX_trackSip3dSig_2', 'FatJet_DDX_trackSip3dSig_3', \\\n",
    "            ]\n",
    "\n",
    "# variables: subjet 1\n",
    "features += ['FatJet_subjet1_pt', 'FatJet_subjet1_eta', 'FatJet_subjet1_phi', \\\n",
    "             'FatJet_subjet1_Proba', 'FatJet_subjet1_tau1', 'FatJet_subjet1_tau2', \\\n",
    "             'FatJet_subjet1_tau3', 'FatJet_subjet1_tau4', 'FatJet_subjet1_n2b1', 'FatJet_subjet1_n3b1', \\\n",
    "            ]\n",
    "\n",
    "# variables: subjet 2\n",
    "features += ['FatJet_subjet2_pt', 'FatJet_subjet2_eta', 'FatJet_subjet2_phi', \\\n",
    "             'FatJet_subjet2_Proba', 'FatJet_subjet2_tau1', 'FatJet_subjet2_tau2', \\\n",
    "             'FatJet_subjet2_tau3', 'FatJet_subjet2_tau4', 'FatJet_subjet2_n2b1', 'FatJet_subjet2_n3b1', \\\n",
    "            ]\n",
    "\n",
    "# variables: fatjet sv\n",
    "features += ['FatJet_sv_costhetasvpv', 'FatJet_sv_d3dsig', 'FatJet_sv_deltaR', 'FatJet_sv_dxysig', \\\n",
    "             'FatJet_sv_enration', 'FatJet_sv_normchi2', 'FatJet_sv_ntracks', 'FatJet_sv_phirel', \\\n",
    "             'FatJet_sv_pt', 'FatJet_sv_ptrel', \\\n",
    "            ]\n",
    "\n",
    "features = sorted(features)\n",
    "\n",
    "original_dim = len(features)\n",
    "\n",
    "inputfile = 'QCD_HT500to700.root'\n",
    "df = get_df(inputfile, '*')\n",
    "\n",
    "df = df[features]\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "X = df.to_numpy()\n",
    "X = X.astype(\"float32\")\n",
    "\n",
    "\n",
    "# standardize the data\n",
    "standard_scaler = StandardScaler()\n",
    "X = standard_scaler.fit_transform(X)\n",
    "# normalize the data to 0 - 1\n",
    "norm_scaler = MinMaxScaler()\n",
    "X = norm_scaler.fit_transform(X)\n",
    "\n",
    "x_train, x_test = train_test_split(X, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dset(df): \n",
    "    df = df.copy()\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(df)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "    \n",
    "x_train_dataset = build_dset(x_train)\n",
    "x_test_dataset = build_dset(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-acrylic",
   "metadata": {},
   "source": [
    "get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_dim_1 = 32\n",
    "intermediate_dim_2 = 16\n",
    "latent_dim = 2\n",
    "\n",
    "with strategy.scope():\n",
    "    encoder = get_encoder(original_dim, intermediate_dim_1, intermediate_dim_2, latent_dim)\n",
    "    decoder = get_decoder(original_dim, intermediate_dim_1, intermediate_dim_2, latent_dim)\n",
    "    model = vae(encoder, decoder)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1.e-3))\n",
    "    #model.compile(optimizer=tf.keras.optimizers.RMSprop())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-premiere",
   "metadata": {},
   "source": [
    "train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_callback():\n",
    "    lr_start   = 0.000001\n",
    "    lr_max     = 0.01\n",
    "    lr_min     = 0.000001\n",
    "    lr_ramp_ep = 5\n",
    "    lr_sus_ep  = 10\n",
    "    lr_decay   = 0.8\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start   \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max    \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "    return lr_callback\n",
    "\n",
    "checkpoint_path = \"weights.{epoch:05d}.hdf5\"\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 monitor = 'val_loss',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only=False,\n",
    "                                                 mode = 'min',\n",
    "                                                 verbose=1)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "history = model.fit(\n",
    "    x_train_dataset,\n",
    "    shuffle=True,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[cp_callback]\n",
    "    #callbacks=[cp_callback, get_lr_callback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-resort",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
