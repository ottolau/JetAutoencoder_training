{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "divided-wealth",
   "metadata": {},
   "source": [
    "import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf, re, math\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras import backend as K \n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n",
    "import uproot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "    print(\"Running on TPU \", tpu.cluster_spec().as_dict()[\"worker\"])\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except ValueError:\n",
    "    print(\"Not connected to a TPU runtime. Using CPU/GPU strategy\")\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-congo",
   "metadata": {},
   "source": [
    "loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y_true, y_pred):\n",
    "    \"\"\" Negative log likelihood (Bernoulli). \"\"\"\n",
    "\n",
    "    # keras.losses.binary_crossentropy gives the mean\n",
    "    # over the last axis. we require the sum\n",
    "    return K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "\n",
    "class KLDivergenceLayer(layers.Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        mu, log_var = inputs\n",
    "\n",
    "        kl_batch = - .5 * K.sum(1 + log_var -\n",
    "                                K.square(mu) -\n",
    "                                K.exp(log_var), axis=-1)\n",
    "\n",
    "        self.add_loss(K.mean(kl_batch), inputs=inputs)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-inclusion",
   "metadata": {},
   "source": [
    "build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(original_dim, intermediate_dim, latent_dim):\n",
    "    # Encoder\n",
    "\n",
    "    x = layers.Input(shape=(original_dim,))\n",
    "    h = layers.Dense(intermediate_dim, activation='relu')(x)\n",
    "\n",
    "    z_mu = layers.Dense(latent_dim)(h)\n",
    "    z_log_var = layers.Dense(latent_dim)(h)\n",
    "\n",
    "    z_mu, z_log_var = KLDivergenceLayer()([z_mu, z_log_var])\n",
    "\n",
    "    # Reparametrization trick\n",
    "    z_sigma = layers.Lambda(lambda t: K.exp(.5*t))(z_log_var)\n",
    "\n",
    "    eps = layers.Input(tensor=K.random_normal(shape=(K.shape(x)[0], \n",
    "                                              latent_dim)))\n",
    "    z_eps = layers.Multiply()([z_sigma, eps])\n",
    "    z = layers.Add()([z_mu, z_eps])\n",
    "\n",
    "    # This defines the Encoder which takes noise and input and outputs\n",
    "    # the latent variable z\n",
    "    encoder = Model(inputs=[x, eps], outputs=z)\n",
    "\n",
    "    # Decoder is MLP specified as single Keras Sequential Layer\n",
    "    decoder = Sequential([\n",
    "        layers.Dense(intermediate_dim, input_dim=latent_dim, activation='relu'),\n",
    "        layers.Dense(original_dim, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    x_pred = decoder(z)\n",
    "\n",
    "    vae = Model(inputs=[x, eps], outputs=x_pred, name='vae')\n",
    "    vae.compile(optimizer='rmsprop', loss=nll)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-leonard",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-newfoundland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(root_file_name, branches):\n",
    "    f = uproot.open(root_file_name)\n",
    "    if len(f.allkeys()) == 0:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.DataFrame(uproot.open(root_file_name)[\"tree\"].arrays(branches))\n",
    "    #df = pd.DataFrame(uproot.open(root_file_name)[\"tree\"].arrays(branches, namedecode=\"utf-8\"))\n",
    "    return df\n",
    "\n",
    "inputfile = 'test.root'\n",
    "df = get_df(inputfile, '*')\n",
    "\n",
    "features = ['Jet_pt']\n",
    "features = sorted(features)\n",
    "\n",
    "X = df[features]\n",
    "x_train, x_test = train_test_split(X, test_size=0.20)\n",
    "\n",
    "x_train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "x_test_dataset = tf.data.Dataset.from_tensor_slices(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-force",
   "metadata": {},
   "source": [
    "get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = len(features)\n",
    "intermediate_dim = 16\n",
    "latent_dim = 8\n",
    "\n",
    "with strategy.scope():\n",
    "    model = get_model(original_dim, intermediate_dim, latent_dim)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-falls",
   "metadata": {},
   "source": [
    "train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_callback():\n",
    "    lr_start   = 0.000001\n",
    "    lr_max     = 0.01\n",
    "    lr_min     = 0.000001\n",
    "    lr_ramp_ep = 5\n",
    "    lr_sus_ep  = 10\n",
    "    lr_decay   = 0.8\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start   \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max    \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "    return lr_callback\n",
    "\n",
    "checkpoint_path = \"weights.{epoch:05d}.hdf5\"\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 monitor = 'val_loss',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only=False,\n",
    "                                                 mode = 'min',\n",
    "                                                 verbose=1)\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "\n",
    "history = model.fit(\n",
    "    x_train_dataset,\n",
    "    x_train_dataset,\n",
    "    shuffle=True,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_test_dataset, x_test_dataset),\n",
    "    callbacks=[cp_callback, get_lr_callback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-diameter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
